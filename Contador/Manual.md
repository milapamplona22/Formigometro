## 1. Setup
using docker
> `make cudnn_tensorflow_opencv-10.2_2.2.0_4.3.0`

## 2. Run Counting

#### 2.1 Check the ID generated by docker
`docker images`

example of my output:
```
REPOSITORY                             TAG                             IMAGE ID            CREATED             SIZE
datamachines/cudnn_tensorflow_opencv   10.2_2.2.0_4.3.0-20200615       d58af4a715a4        14 hours ago        5.86GB
nvidia/cuda                            10.2-cudnn7-devel-ubuntu18.04   2b8bb5f68029        6 days ago          3.82GB
```

#### 2.2 Enter the machine generated by docker (container)
enter docker (d58af4a715a4 is the ID of my image, on my machine, on another machine it may be a different value, replace accordingly in the command below):

> `docker run -ti --rm --gpus all -e DISPLAY=$DISPLAY -v /tmp/.X11-unix/:/tmp/.X11-unix --ipc host --device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidiactl:/dev/nvidiactl --device /dev/nvidia-uvm:/dev/nvidia-uvm  -v /home/:/home/ d58af4a715a4 /bin/bash`

#### 2.3 Run the analysis of a video
> `python yoloVideoDetect.py --video 4_2018-10-04_14-40-00.mp4 --config yolo-voc_1batch_v3.cfg --weights yolo-voc_1batch_final.weights  --classes mila.names --yml pedestrians.yml --show`

#### 2.4 Run the analysis of multiple videos (in parallel), using rush
> `find . -name "*.mp4" | ./rush -j 3 'python yoloVideoDetect.py --video {} --config yolo-voc_1batch_v3.cfg --weights yolo-voc_1batch_final.weights  --classes mila.names --yml {.}.yml'`

In this case, since we are using the GPU, it may not be possible to run many networks on the same GPU at the same time. Testing on an NVIDIA GTX 1050 Ti with 4GB of memory, I managed to run 3 at the same time (I have not tested with more than that yet). On GPUs with less memory (the lab's has 3GB), it may be necessary to test whether it is even possible to run two or more.

#### 2.4.b Videos in sequence (without parallelism), with and without rush
##### 2.4.b.1 with rush
If you just want to specify to analyze multiple videos in sequence, but one at a time (without parallelizing), just replace the number of jobs in the command above with one (`-j 1`).
##### 2.4.b.1 without rush
Or run the above command without rush. This would be possible using `find exec`, but then the command changes a bit.
